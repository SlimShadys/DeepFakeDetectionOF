{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U tqdm gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies loaded.\n",
      "===================================================\n",
      "Cuda available: True\n",
      "GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Total memory: 6.0 GB\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import VGG16_Weights\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    usesColab = True\n",
    "except:\n",
    "    usesColab = False\n",
    "    pass\n",
    "\n",
    "print('Dependencies loaded.')\n",
    "print(\"===================================================\")\n",
    "\n",
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('Cuda available: {}'.format(torch.cuda.is_available()))\n",
    "    print(\"GPU: \" + torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    print(\"Total memory: {:.1f} GB\".format((float(torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)))))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('Cuda not available, so using CPU. Please consider switching to a GPU runtime before running the notebook!')\n",
    "    \n",
    "print(\"===================================================\")\n",
    "print(F\"Torch version: {torch.__version__}\")\n",
    "print(F\"Torchvision version: {torchvision.__version__}\")\n",
    "print(\"===================================================\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Download\n",
    "\n",
    "dataset_dict = {\n",
    "    \"pwc_paperDataset\": '10lDT4ZjuRfYKb4bTNK5gHNf7994bKg4-',    # PWC-Net | Real & Face2Face\n",
    "    \"raft_customDataset\":'1ErjiegdRs7F6iqKD7S4bmVv5-i-RfqPK',   # Raft | Real & Custom Dataset\n",
    "    \"gma_customDataset\":'1T_47yz1E5UlHOl6w8dHcaEZAONdGU3ww',    # GMA | Real & Custom Dataset\n",
    "}\n",
    "\n",
    "Dataset_name = \"pwc_paperDataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = dataset_dict[Dataset_name]\n",
    "output = 'Dataset.zip'\n",
    "\n",
    "datasetDirectory = \"Dataset\"\n",
    "if not (os.path.exists(datasetDirectory)):\n",
    "  os.makedirs(datasetDirectory)\n",
    "\n",
    "dataset_url = 'https://drive.google.com/uc?id=' + ID + '&export=download&confirm=t'\n",
    "gdown.download(dataset_url, output, quiet=False)\n",
    "\n",
    "with zipfile.ZipFile(output) as zf:\n",
    "    for member in tqdm(zf.infolist(), desc='Extracting '):\n",
    "        try:\n",
    "            zf.extract(member, datasetDirectory)\n",
    "        except zipfile.error as e:\n",
    "            pass\n",
    "\n",
    "print('Deleting the previous downloaded zip.')\n",
    "os.remove('Dataset.zip')\n",
    "print('Done.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model (VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "\n",
    "# Replace the last fully connected layer\n",
    "num_features = model.classifier[6].in_features\n",
    "\n",
    "model.classifier[6] = nn.Linear(num_features, 1, device=device)\n",
    "model.classifier.append(nn.Sigmoid())\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"classifier\"):\n",
    "        param.requires_grad = True\n",
    "    if name.startswith(\"features\"):\n",
    "       if int(name.split(\".\")[1]) >= 24:     \n",
    "            param.requires_grad = True\n",
    "       else:\n",
    "           param.requires_grad = False\n",
    "    \n",
    "    # Print the requires_grad property for each parameter\n",
    "    #print(f\"Parameter: {name} | Requires grad: {param.requires_grad}\")    \n",
    "\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "\n",
    "num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "formatted_num_parameters = '{0:,}'.format(num_parameters).replace(',', '.')\n",
    "print(f\"Number of trainable parameters: {formatted_num_parameters}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModel(modelPath, isCheckpoint=True):\n",
    "    bestAcc = 0.0\n",
    "    startEpoch = 1\n",
    "\n",
    "    if(isCheckpoint):\n",
    "        print(\"You specified a pre-loading directory for a checkpoint.\")\n",
    "    else:\n",
    "        print(\"You specified a pre-loading directory for a model.\")\n",
    "\n",
    "    if os.path.isfile(modelPath):\n",
    "        print(\"=> Loading model '{}'\".format(modelPath))\n",
    "        modelLoaded = torch.load(modelPath)\n",
    "\n",
    "        # State & Optimizer\n",
    "        model.load_state_dict(modelLoaded[\"state_dict\"], strict=False)\n",
    "\n",
    "        # Best validations\n",
    "        try:\n",
    "            bestAcc = modelLoaded[\"best_val\"]\n",
    "            valAcc = modelLoaded[\"val_acc\"]\n",
    "            print(F\"\\t- Validation accuracy for this model: {valAcc:.5f}\")\n",
    "            print(F\"\\t- Best validation accuracy: {bestAcc:.5f}\")\n",
    "        except:\n",
    "            print(\"\\t- No best validation accuracy present in this model\")\n",
    "            pass\n",
    "\n",
    "        if(isCheckpoint):\n",
    "            optimizer.load_state_dict(modelLoaded[\"optimizer\"])\n",
    "\n",
    "            # Starting epoch\n",
    "            try:\n",
    "                startEpoch = modelLoaded[\"epoch\"] + 1\n",
    "                print(F\"\\t- Starting from epoch n.{startEpoch}\")\n",
    "            except:\n",
    "                print(\"\\t- No starting epoch present in this model\")\n",
    "                pass\n",
    "\n",
    "        if(isCheckpoint):\n",
    "            print(\"Checkpoint loaded successfully.\")\n",
    "        else:\n",
    "            print(\"model loaded successfully.\")\n",
    "    else:\n",
    "        if(isCheckpoint):\n",
    "            print(\"=> No checkpoint found at '{}'\".format(modelPath))\n",
    "        else:\n",
    "            print(\"=> No model found at '{}'\".format(modelPath))\n",
    "\n",
    "        print(\"Are you sure the directory / model exist? Exiting..\")\n",
    "        raise FileNotFoundError()\n",
    "\n",
    "    print(\"===================================================\")\n",
    "    return bestAcc, startEpoch\n",
    "\n",
    "def saveModel(state, is_best):\n",
    "    if is_best:\n",
    "        path = str(savePath) + '/best/' + F'vgg16_best_epoch-{state[\"epoch\"]}_accT-{state[\"train_acc\"]:.5f}_accV-{state[\"val_acc\"]:.5f}.pt'\n",
    "    else:\n",
    "        path = str(savePath) + '/checkpoint/' + F'vgg16_checkpoint_epoch-{state[\"epoch\"]}_accT-{state[\"train_acc\"]:.5f}_accV-{state[\"val_acc\"]:.5f}.pt'\n",
    "    torch.save(state, path)\n",
    "    return path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters of the training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()  # Cross-entropy loss for classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # Optimizer for training the last layer\n",
    "epochs = 50\n",
    "batchSize = 64\n",
    "# ============================================== #\n",
    "savePath = os.path.join(\"save_path\", \"vgg16\")\n",
    "if not(os.path.exists(savePath)):\n",
    "    os.makedirs(savePath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "transformTrain = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.33),\n",
    "    #transforms.CenterCrop(300),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transformsTest = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define paths for train_dir and test_dir according to split perfomed on OFs dataset\n",
    "train_dir = os.path.join(\"Dataset\", \"train\")\n",
    "test_dir = os.path.join(\"Dataset\", \"test\")\n",
    "\n",
    "# Load the ImageFolder datasets for train and test directories\n",
    "train_dataset = ImageFolder(root=train_dir, transform=transformTrain)\n",
    "test_dataset = ImageFolder(root=test_dir, transform=transformsTest)\n",
    "\n",
    "# Create data loaders for the train and test subsets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchSize, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchSize, shuffle=False)\n",
    "\n",
    "# Check the updated number of frames in each loader\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(\"===================================================\")\n",
    "print(f\"Training feature shape : {train_features.size()}\")\n",
    "print(f\"Training labels shape  : {train_labels.size()}\")\n",
    "print('----------------------------------------------')\n",
    "test_features, test_labels = next(iter(test_loader))\n",
    "print(f\"Test feature shape     : {test_features.size()}\")\n",
    "print(f\"Test labels shape      : {test_labels.size()}\")\n",
    "print(\"===================================================\")\n",
    "print(f\"Lenght train loader: {len(train_loader)}\")\n",
    "print(f\"Lenght test loader: {len(test_loader)}\")\n",
    "print(\"===================================================\")\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Train Features\n",
    "train_image = train_features[0]\n",
    "axes[0].imshow(train_image.permute(1, 2, 0))\n",
    "axes[0].set_title(f\"Train image | Label: {train_labels[0]}\")\n",
    "\n",
    "# Test Features\n",
    "test_image = test_features[0]\n",
    "axes[1].imshow(test_image.permute(1, 2, 0))\n",
    "axes[1].set_title(f\"Test image | Label: {test_labels[0]}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Drive implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(usesColab):\n",
    "  drive.mount('/content/gdrive')\n",
    "\n",
    "  bestPath = os.path.join(os.path.join(\"gdrive\", \"MyDrive\", \"Models\", savePath, \"best\"))\n",
    "  checkPath = os.path.join(os.path.join(\"gdrive\", \"MyDrive\", \"Models\", savePath, \"checkpoint\"))\n",
    "  if not (os.path.exists(bestPath)):\n",
    "    os.makedirs(bestPath)\n",
    "  if not (os.path.exists(checkPath)):\n",
    "    os.makedirs(checkPath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "modelPath = '' # e.g. 'save_path/vgg16/best/vgg16_best_epoch-1_accT-0.61361_accV-0.56974.pt'\n",
    "               # or   '/content/gdrive/MyDrive/X/X/vgg16_best_epoch-1_accT-0.61361_accV-0.56974.pt'\n",
    "isCheckpoint = True\n",
    "\n",
    "if(load_model):\n",
    "  bestAcc, startEpoch = loadModel(modelPath, isCheckpoint=isCheckpoint)\n",
    "else:\n",
    "  bestAcc = 0.00\n",
    "  startEpoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===================================================\")\n",
    "print(F\"The directory for saving checkpoints/models is: {savePath}\")\n",
    "if not(os.path.exists(os.path.join(savePath, 'checkpoint'))):\n",
    "    os.makedirs(os.path.join(savePath, 'checkpoint'))\n",
    "\n",
    "if not(os.path.exists(os.path.join(savePath, 'best'))):\n",
    "    os.makedirs(os.path.join(savePath, 'best'))\n",
    "print(\"===================================================\")\n",
    "\n",
    "trainLosses = []\n",
    "trainAccuracies = []\n",
    "valLosses = []\n",
    "valAccuracies = []\n",
    "    \n",
    "for epoch in range(startEpoch, epochs+1):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    totalTrain = 0\n",
    "    correctTrain = 0\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=F'Epoch {epoch}/{epochs} | Training') as pbar:\n",
    "        for i, data in enumerate(train_loader):\n",
    "            images, labels = data\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            outputs = outputs.view(-1) # Needed for loss computation | Shape [64] instead of [64, 1]\n",
    "            lTrain = criterion(outputs, labels.float())\n",
    "\n",
    "            # Backward and optimize\n",
    "            lTrain.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accuracy\n",
    "            outputs_rounded = (torch.round(outputs)).int()\n",
    "            correctTrain += (outputs_rounded == labels).sum().float()\n",
    "            totalTrain += labels.shape[0]\n",
    "\n",
    "            pbar.set_postfix({'Loss': '{:.3f}'.format(lTrain.item()), 'Accuracy': '{:.3f}'.format((correctTrain / totalTrain)*100)})\n",
    "            pbar.update()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correctVal = 0\n",
    "        totalVal = 0\n",
    "        with tqdm(total=len(test_loader), desc=F'Epoch {epoch}/{epochs} | Test') as pbar:\n",
    "            for i, data in enumerate(test_loader):\n",
    "                images, labels = data\n",
    "\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                outputs = outputs.view(-1) # Needed for loss computation | Shape [64] instead of [64, 1]\n",
    "                lVal = criterion(outputs, labels.float())\n",
    "\n",
    "                # Accuracy\n",
    "                outputs_rounded = (torch.round(outputs)).int()\n",
    "                correctVal += (outputs_rounded == labels).sum().float()\n",
    "                totalVal += labels.shape[0]\n",
    "\n",
    "                pbar.set_postfix({'Loss': '{:.3f}'.format(lVal.item()), 'Accuracy': '{:.3f}'.format((correctVal / totalVal)*100)})\n",
    "                pbar.update()\n",
    "            \n",
    "    # Losses computation\n",
    "    train_loss_epoch = lTrain.item()\n",
    "    val_loss_epoch = lVal.item()\n",
    "    trainLosses.append(train_loss_epoch)\n",
    "    valLosses.append(val_loss_epoch)\n",
    "    \n",
    "    # Accuracies computation\n",
    "    train_acc_epoch = correctTrain / totalTrain\n",
    "    val_acc_epoch = correctVal / totalVal\n",
    "    trainAccuracies.append(train_acc_epoch.detach().cpu().numpy())\n",
    "    valAccuracies.append(val_acc_epoch.detach().cpu().numpy())\n",
    "\n",
    "    if(val_acc_epoch > bestAcc):\n",
    "        isBest = True\n",
    "        previousBestAcc = bestAcc\n",
    "        bestAcc = val_acc_epoch\n",
    "        print(\"\\nVal Accuracy increased at epoch {}: {:.5f} --> {:.5f} |\".format(epoch, previousBestAcc, bestAcc))\n",
    "    else:\n",
    "        isBest = False\n",
    "\n",
    "    pathSaved = saveModel(state={\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'train_acc': train_acc_epoch,\n",
    "                    'val_acc': val_acc_epoch,\n",
    "                    'best_val': bestAcc\n",
    "                }, \n",
    "                is_best=isBest)\n",
    "\n",
    "    print('\\nEpoch {}/{}:\\n\\tTrain Acc: {:.3f} (avg. {:.3f}) | Train Loss: {:.3f} (avg. {:.3f}) | \\\n",
    "        \\n\\tVal Acc  : {:.3f} (avg. {:.3f}) | Val Loss: {:.3f} (avg. {:.3f}) |'.format(epoch, epochs, \n",
    "                                                                                    train_acc_epoch*100,\n",
    "                                                                                    np.average(trainAccuracies)*100,  \n",
    "                                                                                    train_loss_epoch,\n",
    "                                                                                    np.average(trainLosses),\n",
    "                                                                                    val_acc_epoch*100,\n",
    "                                                                                    np.average(valAccuracies)*100,\n",
    "                                                                                    val_loss_epoch,\n",
    "                                                                                    np.average(valLosses)))\n",
    "    print(\"=========================================\")\n",
    "    if (usesColab):\n",
    "        shutil.copy(pathSaved, os.path.join(\"gdrive\", \"MyDrive\", \"Models\", pathSaved))\n",
    "        print(F'Uploaded model in: {os.path.join(\"gdrive\", \"MyDrive\", \"Models\", pathSaved)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
