{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cv2.data import haarcascades\n",
    "from keras.models import load_model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_alphanumeric(data):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(data, key=alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model FaceQnet_v1 for scoring faces ...\")\n",
    "model = load_model(os.path.join(\"models\", \"FaceQnet_v1.h5\"))\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV2 | Haar Cascade method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple images (Main algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\"IMGs\", \"fake\",\"Face2Face\")\n",
    "output = os.path.join(\"output\", \"fake\", \"Face2Face\", \"cropped_faces\")\n",
    "\n",
    "if not (os.path.exists(output)):\n",
    "    os.makedirs(output)\n",
    "\n",
    "output_size = (384, 384)\n",
    "checkpoint = '' # Example: 409_382-frame57.jpg\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(haarcascades +'haarcascade_frontalface_alt.xml')\n",
    "\n",
    "with open('multiple_faces_fake_face2face.txt', 'a') as file:\n",
    "\n",
    "    print(\"Sorting files ...\")\n",
    "    lp = sorted_alphanumeric(os.listdir(path))\n",
    "    listPictures = lp[lp.index(checkpoint):]\n",
    "    print(\"Sort done! Starting detection... \")\n",
    "\n",
    "    # Loop over the images\n",
    "    for image_file in tqdm(listPictures):\n",
    "        i = 0\n",
    "        face_list = []\n",
    "        dim_face_list = []\n",
    "\n",
    "        # Load image\n",
    "        img = (cv2.imread(os.path.join(path, image_file), cv2.IMREAD_COLOR))[:, :, ::-1]\n",
    "        \n",
    "        # read the haarcascade to detect the faces in an image\n",
    "        faces = face_cascade.detectMultiScale(img, 1.1, 4)\n",
    "\n",
    "        # Loop over all detected faces\n",
    "        if len(faces) == 1:\n",
    "            for i, (x, y, w, h) in enumerate(faces):\n",
    "                # To draw a rectangle in a face\n",
    "                face = img[y:y+h, x:x+w]\n",
    "        elif len(faces) > 1:\n",
    "            file.write(image_file + '\\n')\n",
    "            \n",
    "            for i, (x,y,w,h) in enumerate(faces):\n",
    "                face = img[y:y+h, x:x+w]\n",
    "                face_list.append(face)\n",
    "                height, width, channel = face.shape\n",
    "                \n",
    "                resized_face = np.array(cv2.resize(face, (224, 224)), copy=False, dtype=np.float32)\n",
    "                score = model.predict(np.reshape(resized_face, (1, 224, 224, 3)), batch_size=1, verbose=0)\n",
    "                \n",
    "                # Takes only the height because the image are nxn\n",
    "                dim_face_list.append([height, score, face])\n",
    "\n",
    "            max_height_index = max(range(len(dim_face_list)), key=lambda i: dim_face_list[i][0])\n",
    "            max_score_index = max(range(len(dim_face_list)), key=lambda i: dim_face_list[i][1])\n",
    "\n",
    "            if max_height_index == max_score_index:\n",
    "                max_row = dim_face_list[max_height_index]\n",
    "            else:\n",
    "                max_row = max(dim_face_list, key=lambda row: row[0])\n",
    "\n",
    "            face = max_row[2]\n",
    "\n",
    "        else:\n",
    "            file.write(image_file + ' | No face detected\\n')\n",
    "            continue\n",
    "\n",
    "        cv2.imwrite(os.path.join(output, image_file), cv2.resize(face[:, :, ::-1], output_size))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = '' #337-frame72.jpg\n",
    "switchRGB = True\n",
    "#index = 1\n",
    "\n",
    "path = os.path.join(\"IMGs\", \"real\", image)\n",
    "output = os.path.join(\"output\", \"real\", \"cropped_faces\")\n",
    "output_size = (384, 384)\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(haarcascades +'haarcascade_frontalface_alt.xml')               \n",
    "                \n",
    "scores = []\n",
    "face = None\n",
    "\n",
    "# Load image\n",
    "img = (cv2.imread(os.path.join(path), cv2.IMREAD_COLOR))\n",
    "\n",
    "if(switchRGB):\n",
    "    img = img[:, :, ::-1]\n",
    "\n",
    "# Read the haarcascade to detect the faces in an image\n",
    "faces = face_cascade.detectMultiScale(img, 1.1, 10)\n",
    "\n",
    "if len(faces) < 1:\n",
    "    print('No face detected for this image.')\n",
    "elif len(faces) == 1:\n",
    "    for i, (x, y, w, h) in enumerate(faces):\n",
    "        # To draw a rectangle in a face\n",
    "        face = img[y:y+h, x:x+w]\n",
    "else:\n",
    "    for i, (x, y, w, h) in enumerate(faces):\n",
    "        face = img[y:y+h, x:x+w]\n",
    "        print(face.shape)\n",
    "\n",
    "        resized_face = np.array(cv2.resize(face, (224, 224)), copy=False, dtype=np.float32)\n",
    "        score = model.predict(np.reshape(resized_face, (1, 224, 224, 3)), batch_size=1, verbose=0)\n",
    "        print(score)\n",
    "        scores.append([score, face])\n",
    "\n",
    "    max_score = max(scores, key=lambda x: x[0])\n",
    "    #max_score = scores[index]\n",
    "    face = max_score[1]\n",
    "\n",
    "if face is not None:\n",
    "\n",
    "    if(switchRGB):\n",
    "        face = face[:, :, ::-1]\n",
    "\n",
    "    plt.imshow(face)\n",
    "    cv2.imwrite(os.path.join(output, image), cv2.resize(face, output_size))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V1 (dlib) with controls on cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def controlFaceCrop(l, t, r, b, img, fixSize):\n",
    "        if l < 0:\n",
    "            l = 0\n",
    "        if r > img.shape[1]:\n",
    "            r = img.shape[1]\n",
    "        if t < 0:\n",
    "            t = 0\n",
    "        if b > img.shape[0]:\n",
    "            b = img.shape[0]\n",
    "\n",
    "        # Calculate width and height of bounding box\n",
    "        width = r - l\n",
    "        height = b - t\n",
    "\n",
    "        # Determine the scaling factor to resize the bounding box to the fixed size\n",
    "        scale = max(width, height) / max(fixSize)\n",
    "\n",
    "        # Calculate the new dimensions for the bounding box\n",
    "        new_right = int(round(width / scale))\n",
    "        new_bottom = int(round(height / scale))\n",
    "\n",
    "        # Calculate the new top-left corner coordinates for the bounding box\n",
    "        new_left = int(round(l + (width - new_right) / 2))\n",
    "        new_top = int(round(t + (height - new_bottom) / 2))\n",
    "\n",
    "        if new_left < 0:\n",
    "            new_left = 0\n",
    "        if new_right > img.shape[1]:\n",
    "            new_right = img.shape[1]\n",
    "        if new_top < 0:\n",
    "            new_top = 0\n",
    "        if new_bottom > img.shape[0]:\n",
    "            new_bottom = img.shape[0]\n",
    "\n",
    "        # Extract face crop using bounding box\n",
    "        return img[new_top:new_top+new_right, new_left:new_left+new_bottom, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load face detection model\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "path = os.path.join(\"IMGs\", \"real\")\n",
    "\n",
    "fixed_size = (384, 384)\n",
    "\n",
    "checkpoint = '000-frame0.jpg'\n",
    "\n",
    "with open('multiple_faces.txt', 'a') as file:\n",
    "\n",
    "    print(\"Sorting files ...\")\n",
    "    lp = sorted_alphanumeric(os.listdir(path))\n",
    "    listPictures = lp[lp.index(checkpoint):]\n",
    "    print(\"Sort done! Starting detection... \")\n",
    "\n",
    "    # Loop over the images\n",
    "    for image_file in tqdm(listPictures):\n",
    "        scores = []\n",
    "\n",
    "        # Load image\n",
    "        img = cv2.imread(os.path.join(path, image_file), cv2.IMREAD_COLOR)\n",
    "\n",
    "        # Detect faces\n",
    "        faces = detector(img)\n",
    "        \n",
    "        if(len(faces) < 1):\n",
    "            # No faces here\n",
    "            file.write(image_file + ' | No face detected\\n')\n",
    "            continue\n",
    "        elif len(faces) == 1:\n",
    "            for face in faces:\n",
    "                left = face.left()\n",
    "                top = face.top()\n",
    "                right = face.right()\n",
    "                bottom = face.bottom()\n",
    "\n",
    "                face_crop = controlFaceCrop(left, top, right, bottom, img)\n",
    "                \n",
    "                cv2.imwrite(os.path.join(\"output\", \"real\", \"cropped_faces\", image_file),\n",
    "                            cv2.resize(np.array(face_crop, copy=False, dtype=np.float32), fixed_size))        \n",
    "        else:\n",
    "            file.write(image_file + '\\n')\n",
    "\n",
    "            # Loop over each face detected\n",
    "            for face in faces:\n",
    "                \n",
    "                left = face.left()\n",
    "                top = face.top()\n",
    "                right = face.right()\n",
    "                bottom = face.bottom()\n",
    "\n",
    "                # Extract face crop using bounding box\n",
    "                face_crop = controlFaceCrop(left, top, right, bottom, img)\n",
    "\n",
    "                # Resize face crop to (3, 224, 224)            \n",
    "                resized_face = np.array(cv2.resize(face_crop, (224, 224)), copy=False, dtype=np.float32)\n",
    "                score = model.predict(np.reshape(resized_face, (1, 224, 224, 3)), batch_size=1, verbose=0)\n",
    "                scores.append([score, image_file, resized_face])\n",
    "\n",
    "            max_score = max(scores, key=lambda x: x[0])\n",
    "            cv2.imwrite(os.path.join(\"output\", \"real\", \"cropped_faces\", max_score[1]), cv2.resize(max_score[2], fixed_size))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V1 alt. (dlib) without controls on cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load face detection model\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "path = os.path.join(\"IMGs\", \"real\")\n",
    "\n",
    "fixed_size = (384, 384)\n",
    "\n",
    "checkpoint = '000-frame0.jpg'\n",
    "\n",
    "with open('multiple_faces.txt', 'a') as file:\n",
    "\n",
    "    print(\"Sorting files ...\")\n",
    "    lp = sorted_alphanumeric(os.listdir(path))\n",
    "    listPictures = lp[lp.index(checkpoint):]\n",
    "    print(\"Sort done! Starting detection... \")\n",
    "\n",
    "    # Loop over the images\n",
    "    for image_file in tqdm(listPictures):\n",
    "        scores = []\n",
    "\n",
    "        # Load image\n",
    "        img = cv2.imread(os.path.join(path, image_file), cv2.IMREAD_COLOR)\n",
    "\n",
    "        # Detect faces\n",
    "        faces = detector(img)\n",
    "        \n",
    "        if(len(faces) < 1):\n",
    "            # No faces here\n",
    "            file.write(image_file + ' | No face detected\\n')\n",
    "            continue\n",
    "        elif len(faces) == 1:\n",
    "            for face in faces:\n",
    "                left = face.left()\n",
    "                top = face.top()\n",
    "                right = face.right()\n",
    "                bottom = face.bottom()\n",
    "\n",
    "                # Extract face crop using bounding box\n",
    "                face_crop = img[top:top+bottom, left:left+right, :]\n",
    "                \n",
    "                cv2.imwrite(os.path.join(\"output\", \"real\", \"cropped_faces\", image_file),\n",
    "                            cv2.resize(np.array(face_crop, copy=False, dtype=np.float32), fixed_size))        \n",
    "        else:\n",
    "            file.write(image_file + '\\n')\n",
    "\n",
    "            # Loop over each face detected\n",
    "            for face in faces:\n",
    "                \n",
    "                left = face.left()\n",
    "                top = face.top()\n",
    "                right = face.right()\n",
    "                bottom = face.bottom()\n",
    "\n",
    "                # Extract face crop using bounding box\n",
    "                face_crop = img[top:top+bottom, left:left+right, :]\n",
    "\n",
    "                # Resize face crop to (3, 224, 224)            \n",
    "                resized_face = np.array(cv2.resize(face_crop, (224, 224)), copy=False, dtype=np.float32)\n",
    "                score = model.predict(np.reshape(resized_face, (1, 224, 224, 3)), batch_size=1, verbose=0)\n",
    "                scores.append([score, image_file, resized_face])\n",
    "\n",
    "            max_score = max(scores, key=lambda x: x[0])\n",
    "            cv2.imwrite(os.path.join(\"output\", \"real\", \"cropped_faces\", max_score[1]), cv2.resize(max_score[2], fixed_size))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reorder the file about multiple faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No faces detected for 428 frames.\n"
     ]
    }
   ],
   "source": [
    "# Use real_images = True if you are working with images from the \"Real\" class.\n",
    "# Use real_images = False if you are working with images from the \"Fake\" class.\n",
    "real_images = False\n",
    "path = 'multiple_faces_fake_face2face.txt'\n",
    "output = 'final_fake_face2face.txt'\n",
    "\n",
    "# Read the file into a pandas dataframe\n",
    "df = pd.read_csv(path, header=None, names=['string'])\n",
    "\n",
    "if(real_images):\n",
    "    # Extract the video and frame numbers into separate columns\n",
    "    df[['video', 'frame']] = df['string'].str.extract(r'(\\d+)-frame(\\d+)\\.jpg')\n",
    "else:\n",
    "     # Extract the video and frame numbers into separate columns\n",
    "    df[['video', 'fakeVideo', 'frame']] = df['string'].str.extract(r'(\\d+)_(\\d+)-frame(\\d+)\\.jpg')\n",
    "\n",
    "# Convert the video and frame columns to integers\n",
    "df['video'] = df['video'].astype(str)\n",
    "df['frame'] = df['frame'].astype(str)\n",
    "\n",
    "# Group the data by 'video'\n",
    "grouped = df.groupby('video')\n",
    "\n",
    "# Create a list to store the lines for the output file\n",
    "lines = []\n",
    "i = 0\n",
    "\n",
    "# Iterate over each group\n",
    "for video, group in grouped:\n",
    "    frames = []\n",
    "\n",
    "    if not real_images:\n",
    "        video = video.split(\"_\")[0]\n",
    "        \n",
    "    lines.append(f'Video: {video} (Possible multiple faces)')\n",
    "\n",
    "    # Check if any frames have \"No face detected\" message\n",
    "    no_face_frames = group[group['string'].str.contains('No face detected')]\n",
    "\n",
    "    # Add lines for frames with \"No face detected\"\n",
    "    if not no_face_frames.empty:\n",
    "        for _, row in no_face_frames.iterrows():\n",
    "            frames.append(str(row['frame']))\n",
    "            i += 1\n",
    "\n",
    "        result = '-'.join(frames)\n",
    "        lines.append(f'Video: {video} | No face detected at frame {result}.')\n",
    "\n",
    "    lines.append(\"======================================\")\n",
    "\n",
    "# Write the lines to the output file\n",
    "with open(output, 'w') as f:\n",
    "    f.write('\\n'.join(lines))\n",
    "\n",
    "print(F\"No faces detected for {i} frames.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
